<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="我认为对dl这个东西，每人都有自己的职责：scientist实验这些框架能否解决新的问题（太上老君&#x2F;生物学家）；engineer搭建快速框架&#x2F;硬件（怎么造炉子&#x2F;工程师）；mathematicians&#x2F;physics people思考这个炉子的可解释性（永远的好奇宝宝&#x2F;物理学家）；最后data scientist套现有的公式炼丹解决问题（炼丹童子&amp;#">
<meta property="og:type" content="article">
<meta property="og:title" content="炼丹童子的自我修养（更新。。）">
<meta property="og:url" content="https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/index.html">
<meta property="og:site_name" content="VA-11 HALL-B">
<meta property="og:description" content="我认为对dl这个东西，每人都有自己的职责：scientist实验这些框架能否解决新的问题（太上老君&#x2F;生物学家）；engineer搭建快速框架&#x2F;硬件（怎么造炉子&#x2F;工程师）；mathematicians&#x2F;physics people思考这个炉子的可解释性（永远的好奇宝宝&#x2F;物理学家）；最后data scientist套现有的公式炼丹解决问题（炼丹童子&amp;#">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://flappy.name/image.png">
<meta property="og:image" content="https://flappy.name/image-1.png">
<meta property="article:published_time" content="2025-03-30T16:16:38.000Z">
<meta property="article:modified_time" content="2025-03-30T22:13:11.758Z">
<meta property="article:author" content="Shengtao Yao">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://flappy.name/image.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-96x96.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>炼丹童子的自我修养（更新。。）</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
      <link rel="alternate" href="/true" title="VA-11 HALL-B" type="application/atom+xml" />
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 7.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/links/">Links</a></li><!--
     --><!--
       --><li><a href="/academic">Academic Page</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2025/03/30/%E4%BD%BF%E7%94%A8database/%E4%BD%BF%E7%94%A8database/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2025/03/02/2025spring%E5%9B%9E%E5%BF%86%E5%BD%95/2025spring%E5%9B%9E%E5%BF%86%E5%BD%95/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&text=炼丹童子的自我修养（更新。。）"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&title=炼丹童子的自我修养（更新。。）"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&is_video=false&description=炼丹童子的自我修养（更新。。）"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=炼丹童子的自我修养（更新。。）&body=Check out this article: https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&title=炼丹童子的自我修养（更新。。）"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&title=炼丹童子的自我修养（更新。。）"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&title=炼丹童子的自我修养（更新。。）"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&title=炼丹童子的自我修养（更新。。）"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&name=炼丹童子的自我修养（更新。。）&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&t=炼丹童子的自我修养（更新。。）"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB"><span class="toc-number">1.</span> <span class="toc-text">炼丹童子的自我修养</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86data"><span class="toc-number">1.1.</span> <span class="toc-text">处理data</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.1.1.</span> <span class="toc-text">1. 预处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%8F%82%E6%95%B0dataloader%E8%AE%BE%E7%BD%AE%E5%92%8Csample%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.1.2.</span> <span class="toc-text">2. 参数dataloader设置和sample可视化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E6%A1%86%E6%9E%B6"><span class="toc-number">1.2.</span> <span class="toc-text">准备框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.3.</span> <span class="toc-text">训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%B9%E6%96%B9%EF%BC%88%E5%B8%B8%E7%94%A8%E7%9A%84%E6%A1%86%E6%9E%B6nn%EF%BC%9Bpytorch%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">丹方（常用的框架nn；pytorch）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Regression-nn-Linear"><span class="toc-number">2.1.</span> <span class="toc-text">Regression: nn.Linear</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification-nn-Linear-sigmoid-N-softmax"><span class="toc-number">2.2.</span> <span class="toc-text">Classification: (nn.Linear + sigmoid) * N + softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN%E7%9A%84%E6%BC%94%E5%8C%96%EF%BC%9A"><span class="toc-number">2.3.</span> <span class="toc-text">CNN的演化：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Series-Net"><span class="toc-number">2.4.</span> <span class="toc-text">Series Net</span></a></li></ol></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        炼丹童子的自我修养（更新。。）
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">Shengtao Yao</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2025-03-30T16:16:38.000Z" class="dt-published" itemprop="datePublished">2025-03-30</time>
        
      
    </div>


      

      

    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <p>我认为对dl这个东西，每人都有自己的职责：scientist实验这些框架能否解决新的问题（太上老君&#x2F;生物学家）；engineer搭建快速框架&#x2F;硬件（怎么造炉子&#x2F;工程师）；mathematicians&#x2F;physics people思考这个炉子的可解释性（永远的好奇宝宝&#x2F;物理学家）；最后data scientist套现有的公式炼丹解决问题（炼丹童子&#x2F;做题家）；</p>
<p>上dl课每次学完玄乎的丹方总是想着自己实践一下，最后总是发现其实没啥可实践的，除了一些infra的东西（框架&#x2F;infra for ml）engineer可以做做；给data scientist留下的工程实践不难，但是却需要对问题有深刻的理解，套正确的公式。</p>
<p>kaggle是一个很好的训练套公式理解的平台，并且很适合我这种算力不够的学生写一些小比赛练手，遂写博客总结一下pipeline和常用的框架，省的之后思考。</p>
<h2 id="炼丹童子的自我修养"><a href="#炼丹童子的自我修养" class="headerlink" title="炼丹童子的自我修养"></a>炼丹童子的自我修养</h2><h3 id="处理data"><a href="#处理data" class="headerlink" title="处理data"></a>处理data</h3><h4 id="1-预处理"><a href="#1-预处理" class="headerlink" title="1. 预处理"></a>1. 预处理</h4><p>下载data</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">path = <span class="comment"># download </span></span><br><span class="line">os.listdir(path)</span><br></pre></td></tr></table></figure>

<p>例子：minst<br><img src="/image.png" alt="alt text"></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">train_data = pd.read_csv(digit_recognizer_path + <span class="string">&quot;/train.csv&quot;</span>, dtype = np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># split data into features(pixels) and labels(numbers from 0 to 9)</span></span><br><span class="line">targets_numpy = train.label.values</span><br><span class="line">features_numpy = train.loc[:,train.columns != <span class="string">&quot;label&quot;</span>].values/<span class="number">255</span> <span class="comment"># normalization</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train test split. Size of train data is 80% and size of test data is 20%.</span></span><br><span class="line">features_train, features_test, targets_train, targets_test = train_test_split(features_numpy,</span><br><span class="line">targets_numpy,</span><br><span class="line">test_size = <span class="number">0.2</span>,</span><br><span class="line">random_state = <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable</span></span><br><span class="line">featuresTrain = torch.from_numpy(features_train)</span><br><span class="line">targetsTrain = torch.from_numpy(targets_train).<span class="built_in">type</span>(torch.LongTensor) </span><br><span class="line">featuresTest = torch.from_numpy(features_test)</span><br><span class="line">targetsTest = torch.from_numpy(targets_test).<span class="built_in">type</span>(torch.LongTensor) <span class="comment"># data type is long</span></span><br></pre></td></tr></table></figure>

<h4 id="2-参数dataloader设置和sample可视化"><a href="#2-参数dataloader设置和sample可视化" class="headerlink" title="2. 参数dataloader设置和sample可视化"></a>2. 参数dataloader设置和sample可视化</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># batch_size, epoch and iteration</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">n_iters = <span class="number">10000</span></span><br><span class="line">num_epochs = n_iters / (<span class="built_in">len</span>(features_train) / batch_size)</span><br><span class="line">num_epochs = <span class="built_in">int</span>(num_epochs)</span><br><span class="line"></span><br><span class="line">train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)</span><br><span class="line">test = torch.utils.data.TensorDataset(featuresTest,targetsTest)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train, batch_size = batch_size shuffle = <span class="literal">False</span>)</span><br><span class="line">test_loader = DataLoader(test, batch_size = batch_size, shuffle = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">plt.imshow(features_numpy[<span class="number">10</span>].reshape(<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<h3 id="准备框架"><a href="#准备框架" class="headerlink" title="准备框架"></a>准备框架</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 准备i模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegressionModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(LogisticRegressionModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(input_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = <span class="variable language_">self</span>.linear(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型参数。</span></span><br><span class="line">model = LogisticRegressionModel(<span class="number">28</span>*<span class="number">28</span>, <span class="number">10</span>)</span><br><span class="line">error = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参。</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>

<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>ViT的训练例子：</p>
<ol>
<li>准备<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> LambdaLR</span><br><span class="line"></span><br><span class="line">warmup_epochs = <span class="number">10</span></span><br><span class="line">num_epochs = <span class="number">50</span></span><br><span class="line">best_val_acc = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">warmup_cosine_lr</span>(<span class="params">epoch</span>):</span><br><span class="line">    epoch = torch.tensor(epoch, dtype=torch.float32)  <span class="comment"># Cast to tensor</span></span><br><span class="line">    <span class="keyword">if</span> epoch &lt; warmup_epochs:</span><br><span class="line">        <span class="keyword">return</span> (epoch / warmup_epochs).item()  <span class="comment"># Linear warmup</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">0.5</span> * (<span class="number">1</span> + torch.cos((epoch - warmup_epochs) / (num_epochs - warmup_epochs) * torch.pi))).item()</span><br><span class="line"></span><br><span class="line">scheduler = LambdaLR(optimizer, lr_lambda=warmup_cosine_lr)</span><br></pre></td></tr></table></figure></li>
<li>开始训练 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="comment"># ----------------------Training Step-----------------------------</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> _, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">        inputs, labels = data</span><br><span class="line">        inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------------Validation Step-----------------------------</span></span><br><span class="line">    scheduler.step()</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">            images, labels = data</span><br><span class="line">            images, labels = images.to(device), labels.to(device)</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">            total += labels.size(<span class="number">0</span>)</span><br><span class="line">            correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    val_acc = <span class="number">100</span> * correct / total</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, Validation Accuracy: <span class="subst">&#123;val_acc:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> val_acc &gt; best_val_acc:</span><br><span class="line">        best_val_acc = val_acc</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&quot;best_model.pth&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
<p>至此一个简单的流程就结束了。</p>
<h2 id="丹方（常用的框架nn；pytorch）"><a href="#丹方（常用的框架nn；pytorch）" class="headerlink" title="丹方（常用的框架nn；pytorch）"></a>丹方（常用的框架nn；pytorch）</h2><h3 id="Regression-nn-Linear"><a href="#Regression-nn-Linear" class="headerlink" title="Regression: nn.Linear"></a>Regression: nn.Linear</h3><p>ex: <a target="_blank" rel="noopener" href="https://www.kaggle.com/competitions/titanic">https://www.kaggle.com/competitions/titanic</a></p>
<h3 id="Classification-nn-Linear-sigmoid-N-softmax"><a href="#Classification-nn-Linear-sigmoid-N-softmax" class="headerlink" title="Classification: (nn.Linear + sigmoid) * N + softmax"></a>Classification: (nn.Linear + sigmoid) * N + softmax</h3><p>ex: 经典的mnist <a target="_blank" rel="noopener" href="https://www.kaggle.com/competitions/digit-recognizer">https://www.kaggle.com/competitions/digit-recognizer</a></p>
<h3 id="CNN的演化："><a href="#CNN的演化：" class="headerlink" title="CNN的演化："></a>CNN的演化：</h3><ol>
<li><p>lecun&#x2F;alex: conv的诞生<br>(customized nn.conv + activation(relu)) * N + softmax </p>
</li>
<li><p>VGG：越深越好，越深越好啊！<br>(blocks – ) * N + softmax</p>
</li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, out_channels</span>):</span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.LazyConv2d(out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VGG</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, arch, lr=<span class="number">0.1</span>, num_classes=<span class="number">10</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.save_hyperparameters()</span><br><span class="line">        conv_blks = []</span><br><span class="line">        <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> arch:</span><br><span class="line">            conv_blks.append(vgg_block(num_convs, out_channels))</span><br><span class="line">        <span class="variable language_">self</span>.net = nn.Sequential(</span><br><span class="line">            *conv_blks, nn.Flatten(),</span><br><span class="line">            nn.LazyLinear(<span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.LazyLinear(<span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.LazyLinear(num_classes))</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>NiN: 1*1Conv相当于全连接层。1*1Conv具有调整网络层的通道数量、控制模型复杂度的作用</p>
</li>
<li><p>GoogleNet：杂糅block，一个block中用四个不同kernel产生通道数然后进行加和。<br><img src="/image-1.png" alt="alt text"></p>
</li>
</ol>
<p>在这个时代诞生了很多工具：</p>
<ul>
<li>防止过拟合的trick: dropout(P)</li>
<li>下采样：pooling(average&#x2F; max)，降维。</li>
<li>batchnorm&#x2F;layernorm：加入shifting&#x2F;larging factor这两个课学习参数，更快收敛。</li>
</ul>
<ol start="5">
<li>CustomizingNet:<br><a target="_blank" rel="noopener" href="https://d2l.ai/chapter_convolutional-modern/cnn-design.html">https://d2l.ai/chapter_convolutional-modern/cnn-design.html</a></li>
</ol>
<h3 id="Series-Net"><a href="#Series-Net" class="headerlink" title="Series Net"></a>Series Net</h3>
  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a href="/links/">Links</a></li>
        
          <li><a href="/academic">Academic Page</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB"><span class="toc-number">1.</span> <span class="toc-text">炼丹童子的自我修养</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86data"><span class="toc-number">1.1.</span> <span class="toc-text">处理data</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.1.1.</span> <span class="toc-text">1. 预处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%8F%82%E6%95%B0dataloader%E8%AE%BE%E7%BD%AE%E5%92%8Csample%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.1.2.</span> <span class="toc-text">2. 参数dataloader设置和sample可视化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E6%A1%86%E6%9E%B6"><span class="toc-number">1.2.</span> <span class="toc-text">准备框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.3.</span> <span class="toc-text">训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%B9%E6%96%B9%EF%BC%88%E5%B8%B8%E7%94%A8%E7%9A%84%E6%A1%86%E6%9E%B6nn%EF%BC%9Bpytorch%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">丹方（常用的框架nn；pytorch）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Regression-nn-Linear"><span class="toc-number">2.1.</span> <span class="toc-text">Regression: nn.Linear</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification-nn-Linear-sigmoid-N-softmax"><span class="toc-number">2.2.</span> <span class="toc-text">Classification: (nn.Linear + sigmoid) * N + softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN%E7%9A%84%E6%BC%94%E5%8C%96%EF%BC%9A"><span class="toc-number">2.3.</span> <span class="toc-text">CNN的演化：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Series-Net"><span class="toc-number">2.4.</span> <span class="toc-text">Series Net</span></a></li></ol></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&text=炼丹童子的自我修养（更新。。）"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&title=炼丹童子的自我修养（更新。。）"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&is_video=false&description=炼丹童子的自我修养（更新。。）"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=炼丹童子的自我修养（更新。。）&body=Check out this article: https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&title=炼丹童子的自我修养（更新。。）"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&title=炼丹童子的自我修养（更新。。）"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&title=炼丹童子的自我修养（更新。。）"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&title=炼丹童子的自我修养（更新。。）"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&name=炼丹童子的自我修养（更新。。）&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://flappy.name/2025/03/30/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/%E7%82%BC%E4%B8%B9%E7%AB%A5%E5%AD%90%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/&t=炼丹童子的自我修养（更新。。）"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2024-2025
    Shengtao Yao
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/links/">Links</a></li><!--
     --><!--
       --><li><a href="/academic">Academic Page</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
